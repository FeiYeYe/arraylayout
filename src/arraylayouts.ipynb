{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Project Summary\n",
      "\n",
      "This project is about memory layouts for binary search. Using the work of Brodal, Fagerberg, and Jakob (SODA 2002) as a starting point, I wanted to know what is the fastest array layout for binary search. To be specific, I'm only interested in structures where the $n$ data items live in a single array of length $n$.  I'm interested in in-memory performance, which usually boils down to efficient cache use.\n",
      "\n",
      "The candidate layouts I considered were:\n",
      "\n",
      "- **sorted:** the usual sorted array with binary search applied to it;\n",
      "- **eytzinger:** a complete binary search tree layed out in an array using Eytzinger's method;\n",
      "- **btree:** a $B$-tree with B chosen so that B-1 keys fit into a cache line and layed out in an array using a $B$-ary generalization of Eytzinger's method;\n",
      "- **veb:** Prokop's van Emde Boas layout that is so popular from the cache-oblivous literature.\n",
      "\n",
      "I came up with this shortlist based on what I know and the preliminary results from Brodal, Fagerberg, and Jakob. Although I had some preconceptions, I have no stake in this game. None of these are my creation.\n",
      "\n",
      "The sources for this project are available at [github](https://github.com/patmorin/arraylayouts).\n",
      "\n",
      "## Preliminary Implementation\n",
      "\n",
      "I chose C++ as the implementation language: It's close to the the hardware, and C++ templates are a clean way to get generality without sacrificing performance.  The main effects I expected to study here were caching in the L1, L2, L3, and RAM memory hierarchy.  For this, we really needed a language that packs data directly into arrays.\n",
      "\n",
      "I wrote C++ implementations of all four methods described above.  Each method is implemented as a class that can be created from a sorted array.  Each class stores its data in its own array and has a search(x) method that returns the index (in its own array) of the smallest value greater than or equal to x, or returns n if a contains no value greater than x.  A common base class also provides a get_data(i) method that returns the data at index i.\n",
      "\n",
      "The implementations use three way (less than, greater than, and equal to) branching and use early termination in the case of equality.  \n",
      "\n",
      "Most of the implementations were straightforward, with not a lot of design decisions to make. The exception was the veb implementation.  With this implementation I explored the design space a fair bit and eventually settled on an implementation that uses an array of size $h$, where $h=\\min\\{h:2^{h+1}-1 \\ge n\\}$ is the height of the veb tree.  This extra array stores the special values needed to to walk from level $i$ to level $i+1$ in the veb tree.\n",
      "\n",
      "### What I Expected\n",
      "\n",
      "After a surprising false start that has to do with the fact that [binary search is awful when $n$ is close to a power of two](http://www.pvk.ca/Blog/2012/07/30/binary-search-is-a-pathological-case-for-caches/), I formulated some hypotheses about the running-times of these different layouts.\n",
      "\n",
      "My first test machine was lauteschwein, an [Intel 4790K](http://www.cpu-world.com/CPUs/Core_i7/Intel-Core%20i7-4790K.html) with an 8MB L3 cache and 32GB of RAM.  I was mainly interested in large values of $n$, which is then really focused on the interaction between L3 cache and RAM.  I expected the following results. For simplicity, I'll assume 4-byte data and 64-byte cache lines, so we can fit 125K cache lines containing 2M data values into the cache.\n",
      "\n",
      "1. **sorted:** After enough large $n$, the top levels of the search tree can live in the cache, but these all live in different cache lines, so we can get the top $\\log(125K) = 17$ levels to live the cache, so these won't incur any cache misses.  At the end of the search, we finish by searching in a subarray whose length is smaller than 16, so the last four or five steps of the search only incur a single cache miss.  The remaining steps each cost a cache miss. In total, this is about $\\log(n)-\\log 125K-\\log 16=\\log n-21$ cache misses. Call this guess $\\log n-C_1$.  I expected this to be the slowest.\n",
      "\n",
      "2. **eytzinger:** Here, the picture is the same, but the top levels of the tree are consecutive in memory, so the top $\\log(2M)=21$ levels get to live in the cache.  Therefore, we should incur about $\\log(n)-\\log(2M)-\\log(16)=\\log n-25$ cache misses.  I expected this to be just a bit faster than sorted, $\\log n - C_2$.\n",
      "\n",
      "3. **btree:** In this implementation, we took $B=17$ so that the $B-1$ data items for a node fit into a single cache line.  As with eytzinger, we also save on the top $\\log(2M)$ levels of the tree.  In total, we expect about $\\log_{16} n-\\log_{16}(2M)=\\frac{1}{4}\\log n-17$ cache misses.  I expected this to be the fastest, at $\\frac{1}{4}\\log n- C_3$.\n",
      "\n",
      "4. **veb:** For very large $n$, I expected this to be the second fastest, for all the same reasons as the btree, except that the constant $1/4$ is wrong because veb subtrees are not aligned with cache line boundaries (they have size $2^h-1$) and they're not always close to the right size. For example, a subtree of size 31 gets split into a top tree of size 7 and bottom subtrees of size 7, missing the magical 16 by a wide margin. \n",
      "\n",
      "### What Actually Happened\n",
      "\n",
      "The [graphs of results](http://cglab.ca/~morin/misc/arraylayout/lauteschwein/run_data/) surprised me.\n",
      "\n",
      "1. **sorted:** was indeed the slowest.\n",
      "\n",
      "2. **eytzinger:** was tied with btree for first place. This was a big surprise.\n",
      "\n",
      "3. **btree:** was (tied for) the fastest. It also had the noticeably different constant of growth in front of the $\\log n$, as expected.\n",
      "\n",
      "4. **veb:** this was the second fastest for large $n$, but had a lot of overhead for small $n$.  I\n",
      "\n",
      "I was so surprised, that I tried the [same test on a few different machines](http://cglab.ca/~morin/misc/arraylayout/) and even enlisted the help of [reddit r/compsci](http://www.reddit.com/r/compsci/comments/35ad8d/alternatives_to_sorted_arrays_for_binary_searching/). The results were mostly the same, except for a few odd cases. Notably, with the old Atom 330 I used as a home theatre PC. On that machine, [the graphs](http://cglab.ca/~morin/misc/arraylayout/scray/run_data/) were pretty much as expected.\n",
      "\n",
      "It was at this point that [Paul Khuong](http://www.pvk.ca/) contacted to ask how I made my graphs. I had read a few of Paul's blog articles while preparing my code and experiments, so I used the opportunity to engage him in discussion.  \n",
      "\n",
      "Since then, Paul has done a lot of careful benchmarking on a machine he has handy and [rewrote many of the search routines](https://github.com/patmorin/arraylayout/tree/pvk) to make them faster for small values of $n$ and switched out the PRNG for a faster one.\n",
      "\n",
      "### Why is Eytzinger So Fast?\n",
      "\n",
      "The conclusion we eventually reached is that the Eytzinger layout is so fast because of prefetching. After accessing <code>a[i]</code> it always accesses <code>a[2*i+1]</code> or <code>a[2*i+2]</code>.  Modern CPU's are equipped with clever *prefetching algorithms* that can be good at predicting, in advance, which memory locations will be accessed and loading them into cache before they are accessed.  In this case, the prefetcher is detecting the approximate geometric sequence in access patterns and is preloading <code>a[2*i+c]</code>, and maybe even <code>a[4*i+c]</code>, <code>a[8*i+c]</code>, and so on <code>a[8*i+c]</code>.\n",
      "\n",
      "This doesn't work on the Atom 330, probably because it has weak prefetching logic. (It was an underpowered CPU, even when it was introduced in 2008.)  To test this hypothesis, I added a single prefect instruction <code>__builtin_prefetch(16*i+23, 0, 0)</code> at the top of the search loop, and indeed, this [improved the performance](http://cglab.ca/~morin/misc/arraylayout/prefetch-test/xxx/run_data/index.html).\n",
      "\n",
      "### Analysis of Eytzinger\n",
      "\n",
      "The 16 great-great granchildren of index $i$ in an Eytzinger layout are located at positions $16i+15,\\ldots,16i+30$.  If we assume that a begins on the left boundary of a cache line, then $16i+23$ is on the element at index $23 \\bmod 16 = 7$ in a cache line.  This means that if the search proceeds to any of the fifteen great-great grandchildren of $i$ in $16i+16,\\ldots,16i+30$, then that child will be in the same cache line as $16i+23$, which is what we prefetched.\n",
      "\n",
      "In an Eytzinger array, levels 0, 1, 2, and 3 are contained in the first cache line. For levels 4 and beyond, the above analysis shows that we will have prefetched the appropriate cache line 15/16 of the time and we have done so before accessing node $i$, so the prefetcher has had four rounds of search to load the appropriate cache line. \n",
      "\n",
      "\n",
      "### Comparison with B-trees\n",
      "\n",
      "The B-tree implementation only loads a new cache line after each four rounds of binary search, but it doesn't know in advance which cache line it will need until these four iterations are done so, after four rounds of binary search, it stalls waiting on a cache line.\n",
      "\n",
      "The Eytzinger implementation loads a cache line during every round of binary search, but it can wait four rounds for this cache line to load. Thus, if memory bandwidth is unlimited, then B-trees and Eytzinger layouts shoud perform identically, and they do on many architectures.\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Todo List\n",
      "\n",
      "- Try to understand why Paul's code is so much faster for small values of $n$, but significantly slower for large $n$.\n",
      "\n",
      "- Import the pieces of Paul's code that are clear wins. His hard-coded btree inner search seems like one of these.\n",
      "\n",
      "- ~~See what kind of gains we get by aligning Eytzinger~~ I tried this, and it seems not much is gained by forcing the Eytzinger alignment. There is also a good reason to avoid it.  The path to an element of rank $x2^i$ can not have more than a handful of its entries stored in a partially associative cache. This means that we create a situation where locality in the searches can be bad.\n",
      "\n",
      "- ~~Try prefetching two or even four children of a b-tree node and see if that helps.~~  There doesn't seem to be any gains to get from partially prefetching the leaves of a 16-tree.  However, it seems that a 4-tree in which we prefetch the sixteen children is a bit faster than a 16-tree with no prefetching (about 3-5%).  Unfortunately, this doesn't seem like a scalable strategy. What are we going to do with 64-bit data?\n",
      "\n",
      "- Try a version of btrees in which each node is layed out in BFS order. Paul seems to think that cache lines are not loaded atomically, and that they actually come in left to right order.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}