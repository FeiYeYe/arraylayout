\documentclass{patmorin}
\listfiles
\usepackage{amsthm,amsmath,graphicx,wrapfig}
\usepackage{pat}
%\usepackage{coffee4}
\usepackage[letterpaper]{hyperref}
\usepackage[dvipsnames]{color}
\definecolor{linkblue}{named}{Blue}
\hypersetup{colorlinks=true, linkcolor=linkblue,  anchorcolor=linkblue,
citecolor=linkblue, filecolor=linkblue, menucolor=linkblue, pagecolor=linkblue,
urlcolor=linkblue} \setlength{\parskip}{1ex}

\usepackage[skip=0pt]{caption}

\newcommand{\lstlabel}[1]{\label{lst:#1}}
\newcommand{\lstref}[1]{Listing~\ref{lst:#1}}
\newcommand{\Lstref}[1]{\lstref{#1}}

\newcommand{\naive}{na\"{\i}ve}

\usepackage{listings}
\usepackage{minted}
\usepackage{mdframed}
\surroundwithmdframed{minted}

\title{\MakeUppercase{Array Layouts for Comparison-Based Searching}}
\author{Paul-Virak Khuong and Pat Morin}


\begin{document}
\begin{titlepage}
\maketitle


\begin{abstract}
  We attempt to determine the best order in which to store $n$ ordered
  data items in an array, $A$, of length $n$ so that we can, for any query
  value, $x$, quickly find the smallest value in $A$ that is greater
  than or equal to $x$. In particular, we consider the important case
  where there are many such queries to the same array, $A$, which resides
  entirely in RAM.  In addition to the obvious sorted-order we consider
  the Eytzinger (BFS) layout normally used for heaps, an implicit B-tree
  layout that generalizes the Eytzinger layout, and the van Emde Boas
  layout commonly used in the cache-oblivious algorithms literature.

  After extensive testing and tuning on a wide variety of modern hardware,
  we arrive at the conclusion that, for small values of $n$, sorted
  order, combined with a good implementation of binary search is best.
  For larger values of $n$, we arrive at the surprising conclusion that
  the Eytzinger layout is the fastest when executing a single sequence
  of queries.  The latter conclusion is unexpected and goes counter to
  earlier experimental work by Brodal, Fagerberg, and Jakob (SODA~2003),
  who concluded that both the B-tree and van Emde Boas layouts were faster
  than the Eytzinger layout for large values of $n$.  The reason for this
  discrepancy is that, since 2003, processors have changed qualitatively.
\end{abstract}

\end{titlepage}

\pagenumbering{roman}
\tableofcontents
\newpage

\pagenumbering{arabic}
\section{Introduction}

A sorted array combined with binary search represents \emph{the} classic
data structure/query algorithm pair: theoretically optimal, fast in
practice, and discoverable by school children playing guessing games.
Although sorted arrays are \emph{static}---they don't support efficient
insertion or deletion---they still have many practical applications
involving bulk or batch processing of data.  They are useful in this
context because even \naive\ implementations of binary search execute
searches several times faster than the search algorithms of most
popular dynamic data structures such as red-black trees.\footnote{For
example, Barba and Morin \cite{bmXX} found that a naive implementation
of binary search in a sorted array was approximately three times faster
than searching using C++'s \texttt{stl::set} class (implemented as a
red-black tree).}

Every major programming language and environment provides a sorting
routine, so a sorted array is usually just a function call away. Many
language also provide a matching binary search implementation.
In C++, sorting is done with \mintinline{c++}{std::sort()} and binary
search is implemented in \mintinline{c++}{std::lower_bound()} and
\mintinline{c++}{std::upper_bound()}.

The chromium browser code-base, for example, calls these
implementations of binary search from 135 different locations
in a wide variety of contexts, including cookie handling, GUI
layout, graphics and text rendering, video handling, and certificate
management.\footnote{\url{https://goo.gl/zpSdXo}}  This is the code-base
on which Google Chrome, which now boasts over 1 billion users \cite{X},
is based. Improvements to \mintinline{c++}{std::lower_bound()} and
\mintinline{c++}{std::upper_bound()} could improve the user experience
of hundreds of millions of users.

However, sorted order is just one possible layout that can be used to
store data in an array. Other layouts are also possible and---combined
with the right query algorithm---may allow for faster queries.
Other array layouts may be able to take advantage of (or be hurt less
by) modern processor features such as caches, instruction pipelining,
conditional moves, speculative execution, and prefetching.

In this experimental paper we consider four different memory layouts and
accompanying search algorithms.  Our study focuses on the following case:

\begin{itemize}
\item All of the memory layouts we consider store $n$ data items in a
      single array of length $n$.

\item The search algorithms used for each layout can find (say) the
      index of the largest value that is greater than or equal to $x$
      for any value $x$. In case $x$ is greater than any value in the
      array, the search algorithm returns the index $n$.

\item We focus our study on the case where a single thread on a single
      processor executes a large number of queries on the same array.

\item We study real (wall-clock) execution time, no CPU time, cache
      misses, or other proxies for real time.
\end{itemize}

We consider the following four layouts:

\begin{enumerate}
  \item \texttt{Sorted}:  This is the usual sorted layout, in which
  the data is stored in sorted order and searching is done using binary
  search.

  \item \texttt{Eytzinger}: In this layout, the data is viewed as being
  stored in a complete binary search tree and the values of the nodes
  in this virtual tree are placed in the array in the order they would
  be encountered in a left-to-right breadth-first traversal.  The search
  algorithm simulates a search in this implicit binary search tree.

%  The root of this tree is
%  stored at $A[0]$, and the left and right children of the node stored
%  at $A[i]$ are stored at $A[2i+1]$ and $A[2i+2]$, respectively.

  \item \texttt{Btree}: In this layout, the data is viewed as being stored
  in a complete $(B+1)$-ary tree, so that each node stores $B$ values.
  The parameter $B$ is chosen so that $B$ values fit neatly into a single
  cache line and the nodes of this tree are mapped to array locations in
  the order they are encountered in a breadth-first search.

%  using
%  a generalization of the Eytzinger mapping:  For $j\in \{0,\ldots,b\}$,
%  the $j$th child of the node that starts at position $i$ is stored
%  beginning at position $i(b+1)+(j+1)b$.

  \item \texttt{vEB}: In this, the \emph{van Emde Boas} layout,
  the data is viewed as being stored in a complete binary tree
  whose height is $h=\lceil\log (n+1)\rceil -1$. This tree is layed
  out recursively:  If $h=0$, then the single node of the tree is
  stored in $A[0]$.  Otherwise, the tree is split into a top-part
  of height $\lfloor h/2\rfloor$, which is recursively layed in
  $A[0,\ldots,2^{1+\lfloor{h/2\rfloor}}-2]$.  Attached to the leaves of
  this top tree are up to $2^{1+\lfloor{h/2\rfloor}}$ subtrees, which
  are each recursively layed out, in left-to-right order, starting at
  array location $A[2^{1+\lfloor{h/2\rfloor}}-1]$.
\end{enumerate}

\subsection{Related Work}

Brodal, Fagerberg, and Jakob.


These guys look at array layouts of multidimensional arrays within programming languages:

\url{https://engineering.purdue.edu/~mithuna/pubs/ics99.pdf}


\subsection{Summary of Results}

Our findings, with respect to a single process/thread executing repeated random
searches on a single array, $A$, are summarized as follows:

\begin{enumerate}
  \item For small values of $n$ (smaller than the L1 cache), branch-free
    implementations of search algorithms are considerably faster, by nearly
    a factor of two.
  
  \item For small values of $n$ (smaller than the L3 cache), a good
    branch-free implementation of binary search is unbeaten by any other
    strategy.  A branch-free implementation of the Eytzinger layout is a
    close second.
  
  \item For large values of $n$ (larger than the L3 cache), the branch-free
    implementation of binary search is among the worst of all algorithms,
    followed closely by the branch-free implementations of the Eytzinger
    algorithms.
  
  \item For large values of $n$ (larger than the L3 cache), the \naive\
    implementations of search algorithms usually perform better than the
    branch-free implementations.  Branch prediction, even though it is
    incorrect and leads to a pipeline flush 50\% of the time triggers
    the memory subsystem to load a cache line that is correct 50\%
    of the time.  

  \item For large values of $n$, the fastest method is the Eytzinger layout
   combined with a branch-free search that uses explicit prefetching.
\end{enumerate}

It is important to note that our conclusions are limited in scope. In
particular, the preceding conclusions do not hold when multiple threads
are searching simultaneously.  In such cases, the B-tree layout eventually
wins out over the other layouts.

\section{Processor Architecture Considerations}

In this section, we briefly review, at a very high level, the elements of
modern processor architecture that affect our findings.  For concreteness,
we will use numbers from a recent high-end desktop processor, the Intel
4790K \cite{S} with 4 8GB DDR3-1866 memory modules.

\subsection{CPU}

At the highest level, a computer system consists of a processor (CPU)
connected to a random access memory (RAM). On the Intel 4790K, the
CPU runs at frequency of 4GHz, or $4\times10^9$ cycles per second.
This CPU can execute roughly $4\times 10^{9}$ instructions per
second.\footnote{This is only a very rough approximation of the
truth; different instructions have different latencies and throughput
\cite{granlund.instruction}.}

\url{https://gmplib.org/~tege/x86-timing.pdf}


\subsection{RAM, Latency, and Transfer Rate}

The RAM on this system runs at 1866MHz, or roughly $1.866\times10^9$
cycles per second.  This RAM module can transfer 8 bytes per cycle
from RAM to the CPU, for a (theoretical) peak transfer rate of $8\times
1.866\times10^9\approx 15$GB/s.

Individual memory transfers, however, incur \emph{latency}.  The
\emph{(first-word) latency} of this RAM module is approximately 10ns:
From the time a processor issues a request to read a word of memory
until that memory is available is approximately $10^{-8}$ seconds.

Observe that, if the CPU repeatedly reads 4 byte quantities from RAM,
then it receives $10^8$ of these per second, for a transfer rate of
$4\times 10^8=0.4$GB/s.  Note how far this is below this peak transfer
rate of 15GB/s.

This discrepancy is important: If the CPU is executing instructions
that require the contents of memory locations in RAM, and a subsequent
instruction cannot begin before the previous instruction completes,
then the CPU will not execute more than $10^8$ instructions per second;
it will waste approximately 39/40 cycles waiting on data from RAM.

When the CPU reads a RAM address, the RAM moves a 64 byte cache line
into the CPU. From the time CPU issues the read request to the time
the last word of this cache-line is available is approximately 13.4ns.
This quantity is called the \emph{cache-Line (8th-Word) latency}.
If the processor repeatedly reads cache lines from RAM, this results in a
transfer rate of $64 / (13.4\times10^{-9}) \approx 4.8$GB/s.  Observe that
this is still less than a third of the RAM's peak transfer rate.

The key point to take from the preceding discussion is the following. In
order to actually achieve a transfer rate close to the theoretical peak
transfer rate, the CPU must issue memory read requests before previous
requests have finished.  This will be important and is ultimately the
reason that the Eytzinger layout outperforms other layouts; the Eytzinger
layout uses excess bandwidth to reduce the effects of latency.

\subsection{Caches}

Since reading from RAM is a relatively slow operation, processors use
several levels of caches between the processor and the RAM.  When the
CPU reads a memory location, the entire cache line containing that memory
location is loaded into all levels of the CPU cache.  Subsequent accesses
to memory locations in that cache line then occur with less latency
since they can use the data in the CPU caches.

The Intel 4790K has a 32KB L1 data cache (per core), a 256KB L2 cache (per
core), and an 8MB L3 cache (shared among 4 cores).  Each of these cache
levels is successively slower, in terms of both latency and bandwidth,
than the previous level, with L1 being the fastest and L3 being the slowest;
but still much faster than RAM.

\subsection{The Prefetcher}

To help achieve peak memory throughput and avoid having the processor
stall while waiting on RAM, the CPU includes a prefetcher that analyzes
memory access patterns in an attempt to predict future memory accesses.
For instance, in simple code such as the following:

\begin{minted}{c++}
    long sum = 0;
    for (int i = 0; i < n; i++) 
        sum += a[i];
\end{minted}

The prefetcher is likely to detect that memory allocated to array
\mintinline{c++}{a} is being accessed sequentially.  The prefetcher will
then load blocks of \mintinline{c++}{a} into the cache hierarchy even
before they are accessed.  By the time the code actually needs the value
of \mintinline{c++}{a[i]} it will already be available in L1/L2/L3 cache.

Prefetchers on current hardware can detect simple access patterns
like the sequential pattern above.  More generally, they can often
detect arithmetic progressions of the form $a,a+k,a+2k,a+3k,\ldots$
and even interleaved arithmetic progressions such as $a, b, a+k, b+r,
a+2k,b+2r,a+3k,b+3r,\ldots$.  However, current technology does not go
much beyond this.

\subsection{Translation Lookaside Buffer}

As part of modern virtual memory systems, the processor has a
\emph{translation lookaside buffer (TLB)} that maps virtual memory
addresses (visible to processes) to physical memory addresses (addresses
of physical RAM).  Since a TLB is used for every memory access, it is very
fast, and not very large.  TLBs organizes memory into fixed-size pages.
A process that uses multiple pages of memory will sometimes access a
page that is not in the TLB. This is costly, and triggers the processor
to walk the \emph{page table} until it finds the appropriate page and
then it loads the entry for this page into the TLB.

The Intel 4790K has three data TLBs: The first contains 4 entries for
1GB pages, the second contains 32 entries for 2MB pages, and the third
contains 64 entries for 4KB pages.  In our experiments---which were done
on a dedicated system running few other processes---TLB misses were not
a significant factor until the array size exceeded 4GB.


\subsection{Pipelining, Branch-Prediction, and Predicated Instructions}

Executing an instruction on a processor takes several clock cycles, during
which the instruction is (1)~fetched, (2)~decoded, an (3)~effective
address is read (if necessary), and finally the instruction is
(4)~executed.  Since the entire process takes several cycles, this
arranged in a pipeline so that, for example, one instruction is being
executed while the next instruction is reading a memory address, while
the next instruction is being decoded, while the next instruction is
being fetched.

The Nehalem processor architecture, on which the Intel 4790K is based
has a 20--24 stage processor pipeline \cite{X}. If an instruction does
not stall for any other reason, there is still at least a 20--24 cycle
latency between the time an istruction is fetched and until the time it
completes execution.

Processor pipelining works well provided that the CPU knows which
instruction to fetch.  Where this breaks down is in code that contains
\emph{conditional jump} instructions. These instructions will possibly
change the flow of execution based on the result of some previous
comparison.  In such cases, the CPU does not know in advance whether the
following instruction will be the one immediately after the conditional
jump or will be the target of the conditional jump. The CPU has two
options:
\begin{enumerate}
  \item Wait until the condition that determines the target
   of the jump has been tested. In this case, the insruction pipeline
   is not being filled from the time the conditional jump instruction
   enters the pipleline until the the time the jump condition is tested.

  \item Predict whether the jump will occur or not and begin loading
  the instructions from the jump target, or immediately after the jump,
  respectively.  In this case, if the prediction is correct, then no
  time is wasted. If the prediction is incorrect, then once the jump
  condition is finally verified, all instructions placed in the pipeline
  after the conditional jump instruction have to be flushed.
\end{enumerate}

Many processors, including the Intel 4790K, implement the second
option and implement some form of \emph{branch predictor} to perform
accurate predictions.  Branch predictors work well when the condition
is highly predictable so that, e.g., the conditional jump condition is
almost always taken or almost always not taken.

Most modern processors use some form of two-level adaptive predictor
\cite{yeh.patt.two-level} that can even handle second-order statistics,
such as conditional jumps that implement loops with a fixed number of
iterations. In this case, they can detect conditions such as ``this
conditional jump is taken $k$ times consecutively and then not taken
once.''  In standard benchmarks, representative of typical work-loads,
branch-predictor success rates above 90\% and even above 95\% are not
uncommon \cite{X}.

Another useful tool used to avoid branch misprediction (and branches
altogether) is the conditional move \mintinline{nasm}{cmov} family
of instructions.  Introduced into Intel architectures in 1995 with
the Pentium Pro line, these are instructions that move the contents of
one register to another (or to memory), but only if some condition is
satisfied. They do not change the flow of execution and therefore do
not interfere with the processor pipeline.

Conditional moves are a special case of \emph{predicated
instructions}---instructions that are only executed if some predicate
is true.  The Intel IA-64 and ARM CPU architectures include extensive
predicated instruction sets.

\section{The Layouts}

In this section, we provide an in-depth discussion of the implementations
and performance of the four array layouts we tested.

\subsection{Sorted}

Searching a sorted array is an especially important special case, since it
is so widely used.  In this subsection we take special care to understand
the performance of two implementations of binary search on a sorted array.
Even these two simple implementations of binary search already exhibit some
unexpected behaviours on modern processors.

\subsubsection{Cache-Use Analysis}

Here we analyze the cache use of binary search. In this, and all other analyses, we use the following variables:

\begin{itemize}
\item $n$ is the number of data items (the length of the array).
\item $C$ is the cache size, measured in data items.
\item $W$ is the cache-line width, the number of data items that fit into a single cache line.
\end{itemize}

Next we analyze the cache-use of repeated binary searches on the same
array, which is able to take advantage of the cache in two different ways:

\begin{enumerate}
\item (Frequently access values)
After a large number of searches, we expect to find the most frequently
accessed values in the cache.  These are the values at the top of the
(implicit) binary search tree implemented by binary search.  If $n>C/W$,
each of these values will occupy their own cache line, so the cache only
has room for $C/W$ of these frequently accessed values.

\item (Spatial locality)
 Once an individual binary search has reduced the search range down to a
size less than or equal to $W$, the subarray that remains to be searched
will occupy at most one or two cache lines. Thus, on average, we expect
binary search to incur roughly $\log n -\log(C/W) - \log W + 1 = \log n -
\log C + 1$ cache misses.
\end{enumerate}

On the Intel 4790K, whose L3 cache can store 2048K cached values, we
expect to see a sharp increase in search times when $n$ exceeds $2^{21}$,
with each additional level of binary search incurring another L3 cache
miss and access to RAM.  When we plot search time on a vertical axis
versus $n$ on a logarithmic horizontal axis, this shows up as an
increase in slope at approximately $n=2^{21}$.

\subsubsection{Na\"{\i}ve Binary Search}

The source code for \naive\ binary search is shown in
\lstref{nbs}. This code implements binary search for a
value \mintinline{c++}{x} the way it is typically taught in
introductory computer science courses: It maintains a range of indices
$\{\mintinline{c++}{lo},\ldots,\mintinline{c++}{hi}\}$ and at each stage
\mintinline{c++}{x} is compared to the value \mintinline{c++}{a[m]} at
index, \mintinline{c++}{m}, in the middle of the search range. The search
then either finds \mintinline{c++}{x} at index \mintinline{c++}{m} (if
$\mintinline{c++}{x}=\mintinline{c++}{a[m]}$) or continues searching on
one of the ranges $\{\mintinline{c++}{lo},\ldots,\mintinline{c++}{m}\}$
(if $\mintinline{c++}{x}<\mintinline{c++}{a[m]}$) or
$\{\mintinline{c++}{m+1},\ldots,\mintinline{c++}{hi}\}$ (if
$\mintinline{c++}{x}>\mintinline{c++}{a[m]}$).

\begin{listing}
\begin{minted}[linenos]{c++}
template<typename T, typename I>
I sorted_array<T,I>::branchy_search(T x) const {
    I lo = 0;
    I hi = n;
    while (lo < hi) {
        I m = (lo + hi) / 2;
        if (x < a[m]) {
            hi = m;
        } else if (x > a[m]) {
            lo = m+1;
        } else {
            return m;
        }
    }
    return hi;
}
\end{minted}
\caption{Source code for \naive\ binary search.}
\lstlabel{nbs}
\end{listing}


\figref{sorted-i} shows the running time of $2\times 10^6$ searches
for values of $n$ ranging from 1 to $2^{27}$. As the preceding analysis
predicts, there is indeed a sharp increase in slope that occurs at around
$n=2^{21}$.  To give our results a grounding in reality, this graph
also shows the running-time of the \mintinline{c++}{stl::lower_bound()}
implementation---The C++ Standard Template Library implementation
of binary search.  Our \naive\ implementation and the
\mintinline{c++}{stl::lower_bound()} implementation perform nearly
identically.


\begin{figure}
   \centering{\includegraphics{figs/sorted-i}}
   \caption{The running time of \naive\ binary search and \mintinline{c++}{stl::lower_bound()}.}
   \figlabel{sorted-i}
\end{figure}

If we consider only values of $n$ up to $2^{21}$, shown in
\figref{sorted-ii}, we see an additional change in slope at
$n=2^{16}$.  This is the same effect, but at the L2/L3 cache level; the
4790K has a 256KB L2 cache capable of storing $64K=2^{16}$ data items.
Each additional level of binary search beyond that point incurs an
additional L2 cache miss and an access to the L3 cache.

\begin{figure}
   \centering{\includegraphics{figs/sorted-ii}}
   \caption{The running time of \naive\ binary search when all data
    fits into L3 cache.}
   \figlabel{sorted-ii}
\end{figure}


\subsubsection{Branch-Free Binary Search}

Someone with experience in micro-optimizing code will see that, for
modern desktop processors, the code in \lstref{nbs} can be improved
substantially.  There are two problems with this code:

\begin{enumerate}

\item Inside the code is a three-way \mintinline{c++}{if} statement whose
execution path is highly unpredictable. Each of the first two branches
has a close to $50\%$ chance of being executed.  The branch-predictor of
a pipelined processor is forced to guess which of these branches will
occur and load the instructions from this branch into the pipelline.
When it guesses incorrectly (approximately 50\% of the time), the entire
pipeline must be flushed and the instructions for the other branch loaded.

\item The number of iterations of the outer loop is hard to predict. The
loop may terminate early (because \mintinline{c++}{x} was found). Even when
searching for a value \mintinline{c++}{x} that is not present, unless $n$ has
the form $2^k-1$, the exact number of iterations is different for different
values of $x$.  This implies that the branch predictor will frequently
mispredict termination or non-termination of the loop, incurring the
cost of another pipeline flush.

\end{enumerate}

\Lstref{bfbs} shows an alternative implementation of binary search
that attempts to alleviate both problems described above.  In this
implementation, there is no early termination and, for a given array
length \mintinline{c++}{n}, the number of iterations is fixed (because the
value of \mintinline{c++}{n} always decreases by \mintinline{c++}{half}
during each iteration of the loop).  Therefore, when this method is
called repeatedly on the same array, a good branch-predictor will very
quickly learn the number of iterations of the \mintinline{c++}{while}
loop, and it will generate no further branch mispredictions.

\begin{listing}
\begin{minted}[linenos]{c++}
template<typename T, typename I>
I sorted_array<T,I>::_branchfree_search(T x) const {
    const T *base = a;
    I n = this->n;
    while (n > 1) {
        const I half = n / 2;
        base = (base[half] < x) ? &base[half] : base;
        n -= half;
    }
    return (*base < x) + base - a;
}
\end{minted}
\caption{Source code for branch-free binary search.}
\lstlabel{bfbs}
\end{listing}

In the interior of the \mintinline{c++}{while} loop, there is only one
piece of conditional code, which occurs in Line~7.  For readers unfamiliar
with C's choice operator, this code is equivalent to \mintinline{c++}{if
(base[half] < x) base = &base[half]}, so this line either reassigns
the value of \mintinline{c++}{base} (if \mintinline{c++}{base[half]
< x}) or leaves it unchanged.  The compiler implements this using a
\emph{conditional move} (\mintinline{nasm}{cmov}) instruction so that
there is no branching within the while loop.

The use of conditional move instructions to replace branching is
often a topic of heated debate (see, e.g., \cite{X}).  Conditional move
instructions tend to use more clock cycles than traditional instructions
and, in many cases, branch predictors can achieve prediction accuracies
exceeding 95\%, which makes it faster to use a conditional jump.  In this
particular instance, however, the branch predictor will be unable to make
predictions with accuracy exceeding 50\%, making a conditional move the
best choice.  The resulting assembly code, shown in \lstref{bfbs-asm} is
very lean.  The body of the \mintinline{c++}{while} loop is implemented
by Lines~8--15 with the conditional move at Line~12.


\begin{listing}
\begin{minted}[linenos]{nasm}
  .cfi_startproc
  movq    8(%rdi), %rdx       ; move n into rdx
  movq    (%rdi), %r8         ; move a into r8
  cmpq    $1, %rdx            ; compare n and 1
  movq    %r8, %rax           ; move base into rax
  jbe    .L2                  ; quit if n <= 1
.L3:
  movq    %rdx, %rcx          ; put n into rcx
  shrq    %rcx                ; rcx = half = n/2
  leaq    (%rax,%rcx,4), %rdi ; load &base[half] into rdi
  cmpl    %esi, (%rdi)        ; compare x and base[half]
  cmovb   %rdi, %rax          ; set base = &base[half] if x > base[half]
  subq    %rcx, %rdx          ; n = n - half
  cmpq    $1, %rdx            ; compare n and 1
  ja    .L3                   ; keep going if n > 1
.L2:
  cmpl    %esi, (%rax)        ; compare x to *base
  sbbq    %rdx, %rdx          ; set dx to 00..00 or 11...11
  andl    $4, %edx            ; set dx to 0 or 4 
  addq    %rdx, %rax          ; add dx to base
  subq    %r8, %rax           ; compute base - a (* 4)
  sarq    $2, %rax            ; (divide by 4)
  ret
  .cfi_endproc
\end{minted}
\caption{Compiler-generated assembly code for branch-free binary search.}
\lstlabel{bfbs-asm}
\end{listing}

\Figref{sorted-iii} compares the performance of the \naive\ and
branch-free implementations of binary search for array sizes $n$
ranging from 1 to $2^{16}$.  As expected, the branch-free code is much
faster. After accounting for the overhead of the testing harness, the
branch-free search is approximately twice as fast for $n=2^{16}$.

\begin{figure}
   \centering{\includegraphics{figs/sorted-iii}}
   \caption{The running times of \naive\ binary search versus
    branch-free binary search when all data
    fits into L3 cache.}
   \figlabel{sorted-iii}
\end{figure}

However, for larger values of $n$ (shown in \figref{sorted-iv}) the
situation changes.  For $n>2^{16}$, the gap begins to narrow slowly
until $n>2^{21}$, at which point it narrows more quickly.  By the time
time $n$ exceeds $2^{22}$, the branch-free code is slower and the gap
between the two continues to widen from this point onward.

\begin{figure}
   \centering{\includegraphics{figs/sorted-iv}}
   \caption{The running times of \naive\ binary search versus
    branch-free binary search for large values of $n$.}
   \figlabel{sorted-iv}
\end{figure}

The reason for this change in relative performance was not immediately
obvious to us.  After some experimentation, we discovered it comes from
the interplay between branch prediction and the memory subsystem.  In the
\naive\ code, the branch-predictor makes a guess at which branch will
be taken and is correct approximately half the time. An incorrrect guess
causes a costly pipeline flush.  However, a correct guess results in
the memory subsystem starting to load the array location needed during
the next iteration of the \mintinline{c++}{while} loop.

For small $n<2^{16}$, the entire array fits into L2 cache, and the costs
of pipeline flushes exceed any savings obtained by correct guesses.
However, for larger $n$, each correct guess by the branch-predictor
triggers an L2 (in the range $2^{16}<n<2^{21}$) or an L3 (for $n>2^{21}$)
cache miss sooner than it would otherwise.  The costs of these cache
misses are greater than the costs of the pipeline flushes, so eventually,
the branch-free code loses out to the \naive\ code.

Since this was not our initial explanation, and since it was not obvious
from the beginning, we gathered several pieces of evidence to support
or disprove this hypothesis.

\begin{enumerate}
\item Assembly-level profiling showed that, for large $n$, the
  branch-free code spends the vast majority of its time loading from
  memory (Line~11, of \lstref{bfbs-asm}).  This is because the register,
  \mintinline{nasm}{rdi},  containing the memory address to load is the
  target of a conditional move (Line~12).  This conditional move has not
  completed because it is still waiting on the results of the previous
  comparison, so execution stalls at this point.

\item Another possible explanation for our results is that the hardware
   prefetcher is, for some reason, better able to handle the \naive\
   code than the branch-free code.  This seems unlikely, since the memory
   access patterns for both version are quite similar, and probably too
   complicated for a hardware prefetcher to predict. Nevertheless, we
   ruled out this possibility by disabling the hardware prefetcher and
   running the tests again.\footnote{Prefetching was disabled using
   the \mintinline{console}{wrmsr} utility with register number 0x1a4 and value 0xf.  This disables all prefetching \cite{https://software.intel.com/en-us/articles/optimizing-application-performance-on-intel-coret-microarchitecture-using-hardware-implemented-prefetchers}.}    The running-times
   of the code were unchanged by disabling prefetching.

\item We implemented a version of the branch-free code that adds explicit
   prefetching. At the top of the while loop, it
   prefetches array locations \mintinline{c++}{a[half/2]}
   and \mintinline{c++}{a[half+half/2]} using \texttt{gcc}'s
   \mintinline{c++}{__builtin_prefetch()} builtin, which translates into
   the x86 \mintinline{nasm}{prefetch0} instruction.  The performance of
   the resulting code, shown in \figref{sorted-v} is consistent with our
   hypothesis.  The code is nearly as fast as the branch-free code for
   small values of $n$, but tracks (and even improves) the performance
   of the \naive\ code for larger values of $n$.

\begin{figure}
   \centering{\includegraphics{figs/sorted-v}}
   \caption{Branch-free binary search with explicit prefetching is competitive
    for small values of $n$ and a clear winner for large values of $n$.}
   \figlabel{sorted-v}
\end{figure}

   Note that this code actually causes the memory subsystem to do more
   work, and consumes more memory bandwidth since, in general it loads
   two cache lines when only one will be used.  Nevertheless it is faster
   because the memory bandwidth is more than twice the cache line width
   divided by the memory latency.

\item We ran our code on Atom 330 processor that we had available.  This
   low-power processor does not do any form or speculative
   execution, including branch-prediction. The results, which are
   shown in \figref{sorted-atom} are consistent with our hypothesis.
   The branch-free code is faster than the \naive\ code across the full
   range of values for $n$.  This is because, on the Atom 330, branches
   in the \naive\ code result in pipeline stalls and do not help the
   memory subsystem predict future memory accesses.
\end{enumerate}

\begin{figure}
   \centering{\includegraphics{figs/sorted-atom}}
   \caption{Binary search running-times on the Atom 330. The Atom 330
   has a 512KB L2 cache that can store $2^{17}$ 4-byte integers and does
   not perform speculative execution of any kind.}
   \figlabel{sorted-atom}
\end{figure}

From our study of binary search, we conclude the following lesson about
the (very important) case where one is searching in a sorted array:

\begin{lesson}
  For searching in a sorted array, the fastest method uses branch-free
  code with explicit prefetching.  This is true both on modern pipelined
  processors that use branch prediction and on traditional sequential
  processors.
\end{lesson}

We finish our discussion of binary search by pointing out one lesser-known
caveat:  If $n$ is large and very close to a power of 2, then all three
variants of binary search discussed here will have poor cache utilization.
This is caused by cache-line aliasing between the elements at the top
levels of the binary search.  This problem is hinted at by Brodal \etal\
\cite[Section~XX]{X} and discussed in detail by the first author \cite{X},
who also suggests workarounds.

\subsection{Eytzinger}

Recall that, in the Eytzinger layout, the data is viewed as being
stored in a complete binary search tree and the values of the nodes in
this virtual tree are placed in the array in the order they would be
encountered in a left-to-right breadth-first traversal.  A nice property
of this layout is that it is easy to follow a root-to-leaf path in the
virtual tree: The value of the root of the virtual tree is stored at
index 0, and the values of left and right children of the node stored
at index $i$ are stored at indices $2i+1$ and $2i+2$, respectively.


\subsubsection{Cache-Use Analysis}

With respect to caching, the performance of searches in an Eytzinger
array should be comparable to that of binary searching in a sorted array,
but for slightly different reasons.

When performing repeated random searches, the top levels of the (virtual)
binary tree are accessed most frequently, with a node at depth $i$
being accessed with probability roughly $1/2^i$.  Since nodes of the
virtual tree are mapped into the array in breadth-first order, the
top levels appear consecutively at the beginning of the array. Thus,
we expect the cache to store, roughly, the first $C$ elements of the
array, which correspond to the top $\log C$ levels of the virtual tree.
As the search proceeds through the top $\log C$ levels, it hits cached
values, but after $\log C$ levels, each subsequent comparison causes
a cache miss.  This results in a total of roughly $\log n-\log C$ cache
misses, just as in binary search.

\subsubsection{Na\"{\i}ve Eytzinger Search}

The \naive\ implementation of search in an Eytzinger array is nearly
as simple as that of binary search; the only additional complication is
the use of a variable, \mintinline{c++}{j} to keep track of the index
of the smallest value encountered that is greater than \mintinline{c++}{x}.
In case \mintinline{c++}{x} is not found, the index \mintinline{c++}{j}
will be returned instead.

\begin{listing}
\begin{minted}[linenos]{c++}
template<typename T, typename I, bool aligned>
I eytzinger_array<T,I,aligned>::_branchy_search(T x) const {
	I i = 0, j = n;
	while (i < n) {
		if (x < a[i]) {
			j = i;
			i = 2*i + 1;
		} else if (x > a[i]) {
			i = 2*i + 2;
		} else {
			return i;
		}
	}
	return j;
}
\end{minted}
\caption{A \naive\ implementation of search in an Eytzinger array.}
\end{listing}

\subsubsection{Branch-Free Eytzinger Search}

A branch-free implementation of search in an Eytzinger array is also
quite simple. Unfortunately, unlike branch-free binary search, the
branch-free Eytzinger search is unable to avoid variations in the number
of iterations of the while loop, since this depends on the depth of the
leaf that is reached when searching for \mintinline{c++}{x}.

\begin{listing}
\begin{minted}[linenos]{c++}
template<typename T, typename I, bool aligned>
I eytzinger_array<T,I,aligned>::_branchy_search(T x) const {
    I i = 0, j = n;
    while (i < n) {
        j = (x <= a[i]) ? i : j;
        i = (x <= a[i]) ? (2*i + 1) : (2*i + 2);
    }
    return j;
}
\end{minted}
\caption{Branch-free implementation of search in an Eytzinger array.}
\end{listing}

The performance of the \naive\ and branch-free versions of search in
an Eytzinger array on our test system is shown in \figref{eytzinger-i}.
Branch-free Eytzinger search very closely matches the performance of
branch-free binary search. As expected, there is an increase in slope at
$n=2^{16}$ and $n=2^{21}$ since these are the number of 4-byte integers
that can be stored int the L2 and L3 caches, respectively.

However, the performance of the \naive\ Eytzinger implementation is
much better than expected for large values of $n$. Indeed, it is much
faster than the \naive\ implementation of binary search, and even
faster our fastest implementation of binary search: branch-free binary
search with prefetching.

\begin{figure}
   \centering{\includegraphics{figs/eytzinger-i}}
   \caption{The performance of \naive\ and branch-free Eytzinger search.}
   \figlabel{eytzinger-i}
\end{figure}

\subsubsection{Why Eytzinger is so Fast}

Like \naive\ binary search, the \naive\ implementation of search in
Eytzinger array is so much faster than expected because of the interaction
between branch-prediction and the memory subsystem.  However, in the
case of the Eytzinger layout, this interaction is much more efficient.

To understand why the \naive\ Eytzinger search algorithm is so
fast, recall that, in the Eytzinger layout, the nodes are layed out
in breadth-first search order,  so the two children of a node occupy
consecutive array locations $2i$ and $2i+1$. More generally, for a
virtual node, $u$, that is assigned index $i$, the $2^\ell$ descendants at
distance $\ell$ from $u$ are consecutive and occupy array indices $2^\ell
i + 2^{\ell}-1,\ldots,2^{\ell} i + 2^{\ell+1}-1$.

In our working example of 4-byte data with 64-byte cache lines,
the sixteen great-great grandchildren of the virtual node stored at
position $i$ are stored at array locations $16i+15,\ldots,16i+30$.
Assuming that $A[0]$ is the first element of a cache line, this means
that, of those 16 descendants, 15 are contained in a single cache line
(the ones stored at array indices $16i+16,\ldots,16i+30$.)

With the Intel 4790K's 20--24 instruction pipleline, instructions
in the pipeline can be several iterations ahead in the execution of
the while loop, which only requires 5--6 instructions per iteration.
The probability that the branch-predictor correctly guesses the execution
path of four consecutive stages is only about 1/16, and a flush of (some
of) the pipeline is likely.  However, even if the branch predictor does
not guess the correct execution path, it most likely (with probability
15/16) guesses an execution path that loads the correct cache line.
Even though the instruction will likely never be executed, its presence
in the pipeline causes the memory subystem to begin loading the cache
line that will eventually be needed.

Knowing this, there are two optimizations we can make:
\begin{enumerate}
  \item We can add an explicit prefetch instruction to the branch-free code
    so that it loads the correct cache line.
  \item We can align the array so that \mintinline{c++}{a[0]} is the
    second element of a cache line.  In this way, all 16 great-great
    grandchildren of a node will always be stored in a single cache line.
\end{enumerate}

The results of implementing both of these optimizations are shown in
\figref{eytzinger-ii}.  With these optimizations, the change in slope
at the L2 cache limit disappears; four iterations of the search loop
is enough time to prefetch data from the L3 cache.  Once the L3 cache
limit is reached, the behaviour becomes similar to the \naive\
implementation, but remains noticeably faster, since the branch-free
code does not cause pipeline flushes and always prefetches the correct
cache line 4 levels in advance.

\begin{figure}
   \centering{\includegraphics{figs/eytzinger-ii}}
   \caption{The performance of branch-free Eytzinger with prefetching.}
   \figlabel{eytzinger-ii}
\end{figure}

\subsection{BTree}

Recall that the $(B+1)$-tree layout simulates search on a $(B+1)$-ary
search tree, in which each node stores $B$ values.  The nodes of this
$(B+1)$-ary tree are layed out in the order they are encountered in a
breadth-first search.  Note that the Eytzinger layout is a special case
of this layout in which $B=1$.  For $j\in\{0,\ldots,B\}$, the $j$-th
child of the node stored at indices $i,\ldots,i+B-1$ is stored at
indices $f(i),\ldots,f(i)+B-1$ where $f(i)=i(B+1)+(j+1)B$

The code for search in a $(B+1)$-tree layout consists of several
iterations of binary search, each on a subarray (block) of length
$B$. Very occasionally, the BTree search completes with one binary search
on a block of size less than $B$. The block size, $B$, is chosen to fit
neatly into one cache line. On our test machines, cache lines are 64
bytes wide, so we chose $B=16$ for 4-byte data. Preliminary testing with
other block sizes showed that this theoretically optimal choice for $B$
is also optimal in practice.


\subsubsection{Cache-Use Analysis}

The height of a $(B+1)$ tree that stores $n$ keys is approximately
$\log_{(B+1)} n$.  A search in a $(B+1)$-tree visits $\log_{(B+1)} n$
nodes, each of which is contained in a single cache line, so a search
requires loading onl$\log_{(B+1)} n$ cache lines.

By the same reasoning as before, we expect that the top $\log_{(B+1)} C$
levels of the $(B+1)$-tree, which correspond to the first $C$ elements
of the array, will be stored in the cache.  Thus, the number of cache
misses that occur when searching in a $(B+1)$-tree is
\[
    \log_{(B+1)}n - \log_{(B+1)} C 
         = \frac{\log n - \log C}{\log (B+1)} \enspace .
\]
Observe that this is roughly a factor of $\log(B+1)$ fewer cache misses
than the $\log n-\log C$ cache misses incurred by binary search or
Eytzinger search.

In our test setup, with $B=16$, we therefore expect that cache misses
should be reduced by a factor of $\log 17\approx 4$. When plotted,
there should still be an increase in slope that occurs at $n=2^{21}$,
but it should be significantly less pronounced (a factor of 4).

\subsubsection{Na\"{\i}ve and Branch-Free BTree Implementations}

\Figref{btree-i} shows the results for \naive\ and branch-free
implementations of $17$-trees alongside branch-free binary search
and our fastest implementation of Eytzinger search.  Compared to
branch-free binary search, the 17-tree implementation behaves as the
cache-use analysis predicts:  Both algorithms are fast for $n<2^{21}$
after which both show an increase in slope. At this point, the the slope
for branch-free binary search is approximately 3.8 times larger than
that of the 17-tree search.


our best implementations of binary
search and Eytzinger search.  Both implementations of 17-trees use C++
template programming to unroll the inner binary search.  There is very
little difference between the two implementations of 17-trees and both
perform worse than the Eytzinger search across the entire spectrum of
values of $n$. Unsurprisingly, 17-trees outperform binary search once
the size of the data exceeds the size of the L3 cache.

\begin{figure}
   \centering{\includegraphics{figs/btree-i}}
   \caption{The performance of 17-trees.}
   \figlabel{btree-i}
\end{figure}

The reason 17-tree layout does not perform better than the Eytzinger
layout is that it is unable to take advantage of implicit or explicit
prefetching.  Once the search algorithm leaves the cache (after $\log_{(B+1)} C$ levels), it repeatedly reads a cache line and then does binary search on the elements of that cache line before loading the next cache line.



17-trees, which are theoretically the most promising, unfortunately lose
out simply because the constants they incur are too large.  One reason
for this is that their inner binary search is inherently inefficient. The
search has 17 possible outcomes, and therefore requires $\lceil\log_2
17\rceil=5$ comparisons in some cases (and in all cases for the branch
free code).  This could be fixed by using 16-trees instead, but then the
number of keys stored in a block would be 15, which does not align with
cache lines.  We tested 16-trees and found them to be slower than 17-trees
for all values of $n$.\footnote{A third alternative is to store 15 keys
in blocks size 16, but this would be outside the model we consider,
in which all data resides in a single array of length $n$.}

Unlike the Eytzinger layout, the BTree layout is unable to take much
advantage of implicit or explicit prefetching.  

In summary, $(B+1)$-trees must make a choice between being efficient in
terms of the number of comparisons or being efficient in terms of the
the number of cache misses.  In external memory applications where cache
misses are orders of magnitude slower than comparisons, this is an easy
choice. For internal memory, unfortunately, there is no good choice:
both are slower than a fast Eytzinger implementation.


In external memory applications---for which B-trees were originally designed---it is clear


Their inner binary search is inefficient.


The performance
We implemented several variations of BTree search:

several variations of BTree searches:

Some also tried:  BTree with Eytzinger layouts.

\subsection{Van Emde Boas}


\subsection{Blocked Eytzinger Arrays}


\section{Lessons Learned}

\subsection{Predictable memory accesses can be better than blocked
            memory accesses}

\subsection{For big data, branchy code can be the best prefetcher}

In here, discuss the Sun Rock's hardware scout and, more generally, the technique of Runahead: \url{http://users.ece.cmu.edu/~omutlu/pub/mutlu_hpca03.pdf}.

\section{Limitations}

With parallel searches, we run out of bandwidth.

\section{Conclusion}

Future Work: Look more closely still at the important special case of searching
in a sorted array. We only used a simple prefetching strategy. Mix this
in with a solution to the powers of two problem.

\end{document}


