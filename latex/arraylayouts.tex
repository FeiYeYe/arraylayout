\documentclass{patmorin}
\listfiles
\usepackage{amsthm,amsmath,graphicx,wrapfig}
\usepackage{pat}
%\usepackage{coffee4}
\usepackage[letterpaper]{hyperref}
\usepackage[dvipsnames]{color}
\usepackage{listings}
\definecolor{linkblue}{named}{Blue}
\hypersetup{colorlinks=true, linkcolor=linkblue,  anchorcolor=linkblue,
citecolor=linkblue, filecolor=linkblue, menucolor=linkblue, pagecolor=linkblue,
urlcolor=linkblue} \setlength{\parskip}{1ex}

\newcommand{\lstlabel}[1]{\label{lst:#1}}
\newcommand{\lstref}[1]{Listing~\ref{lst:#1}}

\usepackage{minted}
\usepackage{mdframed}
\surroundwithmdframed{minted}

\title{\MakeUppercase{Array Layouts for Comparison-Based Searching}}
\author{Pat Morin and Paul-Virak Khuong}


\begin{document}
\begin{titlepage}
\maketitle

\begin{abstract}
  This experimental work studies the best order in which to store $n$
  data items in an array, $A$, of length $n$ so that one can, for any
  query value, $x$, quickly find the smallest value in $A$ that is greater
  than or equal to $x$. In particular, we consider the important case
  where there are many such queries to the same array, $A$.  In addition
  to the obvious sorted-order we consider the Eytzinger (BFS) layout
  normally used for heaps, an implicit B-tree layout that generalizes
  the Eytzinger layout, and the van Emde Boas layout commonly used in
  the cache-oblivious algorithms literature.

  After extensive testing and tuning on a wide variety of modern hardware,
  we arrive at the conclusion that, for small values of $n$, sorted
  order, combined with a good implementation of binary search is best.
  For larger values of $n$, we arrive at the surprising conclusion that
  the Eytzinger layout is the fastest when executing a single sequence
  of queries.  The latter conclusion was unexpected and constradicts
  earlier experimental work by Brodal, Fagerberg, and Jakob (SODA~2003),
  who concluded that both the B-tree and van Emde Boas layouts were faster
  than the Eytzinger layout for large values of $n$.  The reason for this
  discrepancy is that, since 2003, processors have changed qualitatively.
\end{abstract}

\end{titlepage}

\section{Introduction}

A sorted array combined with binary search represents \emph{the} classic
data structure/query algorithm pair: theoretically optimal, fast in
practice, and discoverable by school children playing guessing games.
Although sorted arrays are \emph{static}---they don't support efficient
insertion or deletion---they still have many practical applications
involving bulk-processing of data, and even the most na\"{\i}ve
implementations execute searches several times faster than the most
popular dynamic data structures.\footnote{For example, Barba and
Morin \cite{bmXX} found that a naive implementation of binary search
in a sorted array was approximately three times faster than searching
using C++'s \texttt{stl::set} class.}

Note, however, that sorted order is just one possible order that can
be used to store data in an array. Other layouts are also possible
and---combined with the right query algorithm---may allow for faster
queries.  Other array layouts may be able to take advantage of (or be
hurt less by) modern processor features such as caches, instruction
pipelining, conditional moves, speculative execution, and prefetching.

In this paper we consider four different memory layouts and accompanying search algorithms:

\begin{enumerate}
\item \texttt{Sorted}:  This is the usual sorted layout, in which the data is stored in sorted order and searching is done using binary search.

\item \texttt{Eytzinger}: In this layout, the data is viewed as being
stored in a complete binary search tree.  The root of this tree is stored
at $A[0]$, and the left and right children of the node stored at $A[i]$
are stored at $A[2i+1]$ and $A[2i+2]$, respectively.

\item \texttt{Btree}: In this layout, the data is viewed as being stored
in a complate $(b+1)$-ary tree, so that each node stores $b$ values.
The parameter $b$ is chosen so that $b$ values fit neatly into a
single cache line. The nodes of this tree are then mapped to array
locations using a generalization of the Eytzinger mapping:  For $j\in
\{0,\ldots,b\}$, the $j$th child of the node that starts at position $i$
is stored beginning at position $i(b+1)+(j+1)b$.

\item \texttt{vEB}: In this, the \emph{van Emde Boas} layout, the data
is viewed as being stored in a complete binary tree whose height is
$h=\lceil\log (n+1)\rceil -1$. This tree is layed out recursively:  If $h=0$,
then the single node of the tree is stored in $A[0]$.  Otherwise, the tree is split into a top-part of height $\lfloor h/2\rfloor$, which is recursively layed out starting at $A[0]$.  Attached to the leaves of this top tree are $2^{1+\lfloor{h/2\rfloor}}$ subtrees, which are recursively layed out, in left-to-right order, starting at array location $A[2^{1+\lfloor{h/2\rfloor}}-1]$.
\end{enumerate}


\subsection{Related Work}

Brodal, Fagerberg, and Jakob.


These guys look at array layouts of multidimensional arrays within programming languages:

\url{https://engineering.purdue.edu/~mithuna/pubs/ics99.pdf}


\subsection{Summary of Results}

Our findings, with respect to a single process executing repeated random
searches on a single array, $A$, are summarized as follows:

\begin{enumerate}
  \item For small values of $n$ (smaller than the L1 cache), branch-free
    implementations of search algorithms are considerably faster, by nearly
    a factor of two.
  
  \item For small values of $n$ (smaller than the L3 cache), a good
    branch-free implementation of binary search is unbeaten by any other
    strategy.  A branch-free implementation of the Eytzinger layout is a
    close second.
  
  \item For large values of $n$ (larger than the L3 cache), the branch-free
    implementation of binary search is among the worst of all algorithms,
    followed closely by the branch-free implementations of the Eytzinger
    algorithms.
  
  \item For large values of $n$ (larger than the L3 cache), the branchy
    implementations of search algorithms usually perform better than the
    branch-free implementations.  Branch prediction, even though it is
    incorrect and leads to a pipeline flush 50\% of the time triggers
    the memory subsystem to load a cache line that is correct 50\%
    of the time.  

  \item For large values of $n$, the fastest method is the Eytzinger layout
   combined with a branch-free search that uses explicit prefetching.
\end{enumerate}

\section{Processor Architecture Considerations}

In this section, we briefly review, at a very high level, the elements of
modern processor architecture that affect our findings.  For concreteness,
we will use numbers from a recent high-end desktop processor, the Intel
4790K \cite{S} with 4 8GB DDR3-1866 memory modules.

\subsection{CPU}

At the highest level, a computer system consists of a processor (CPU)
connected to a random access memory (RAM). On the Intel 4790K, the CPU
runs at frequency of 4GHz, or $4\times10^9$ cycles per second.  This CPU
can execute roughly $4\times 10^{9}$ instructions per second.\footnote{This is only a very rough approximation of the truth; different instructions have different latencies and throughput \cite{granlund.instruction}.}
%https://gmplib.org/~tege/x86-timing.pdf 


\subsection{RAM, Latency, and Transfer Rate}

The RAM, on ths system runs at 1866MHz, or roughly $1.866\times10^9$
cycles per second.

Transfer rate: This RAM module can transfer 8 bytes per cycle from RAM
to the CPU, for a (theoretical) peak transfer rate of 
$8\times 1.866\times10^9\approx 15$GB/s.

First-Word Latency: This RAM module has a \emph{first-word latency} of
approximately 10ns: From the time a processor issues a memory request
to that memory is available is approximately $10^{-8}$ seconds.

Observe that, if the CPU repeatedly reads 4 byte quantities from RAM,
then it receives $10^8$ of these per second, for a transfer rate of
$4\times 10^8=0.4$GB/s.  Note how far this is below this peak transfer
rate of 15GB/s.

In a similar vein, if the CPU is executing instructions that require
the contents of memory locations in RAM, and a subsequent instruction
can not begin before the previous instruction completes, then the CPU
will not execute more than $10^8$ instructions per second; it will waste
approximately 39/40 cycles waiting on data from RAM.

Cache-Line (8th-Word) Latency:  When the CPU reads a RAM address, the
RAM moves a 64 byte cache line into the CPU. From the time CPU issues
the access to the time the last word of the cache-line is available is
approximately 13.4ns.  If the processor repeatedly reads cache lines
from RAM, this results in a transfer rate of $64 / (13.4\times10^{-9})
\approx 4.8$GB/s.  Observe that this is still less than a third of the
RAM's peak transfer rate.

To actually achieve a transfer rate close to the theoretical peak transfer
rate, the CPU must issue memory read requests before previous requests
have finished.  This will be important and is ultimately the reason that
the Eytzinger layout outperforms other layouts.

\subsection{Caches}

Since reading from RAM is a relatively slow operation, processors use
several levels of caches between the processor and the RAM.

The Intel 4790K has a 32KB L1 data cache (per core), a 256KB L2 cache
(per core), and an 8MB L3 cache (shared among 4 cores).  

\subsection{Translation Lookaside Buffer}

\subsection{Pipelining}

Executing an instruction on a processor takes several clock cycles, during
which the instruction is (1)~fetched, (2)~decoded, an (3)~effective
address is read (if necessary), and finally the instruction is
(4)~executed.  Since the entire process takes several cycles, this
arranged in a pipeline so that, for example, one instruction is being
executed while the next instruction is reading a memory address, while
the next instruction is being decoded, while the next instruction is
being fetched.

This all works well provided 
Where pipelining breaks down

\section{The Layouts}

\subsection{Sorted}

\subsubsection{Na\"{\i}ve Binary Search}

The source code for na\"{\i}ve binary search is shown in
\lstref{nbs}. This code implements binary search the way it is taught
in introductory computer science courses.

\begin{listing}
\begin{minted}{c++}
template<typename T, typename I>
I sorted_array<T,I>::branchy_search(T x) const {
    I lo = 0;
    I hi = n;
    while (lo < hi) {
        I m = (lo + hi) / 2;
        if (x < a[m]) {
            hi = m;
        } else if (x > a[m]) {
            lo = m+1;
        } else {
            return m;
        }
    }
    return hi;
}
\end{minted}
\caption{Source code for na\"{\i}ve binary search.}
\lstlabel{nbs}
\end{listing}


$C$ is the cache size (measured in data items) and $W$ is the cache-line width (measured in data items).  Caching affects classic binary search in two ways.

If $n> C/W$ is much larger than $C$ then, after a large number of
accesses, we expect to find the most frequently accessed values in the
cache.  These are the values at the top of the (implicit) binary search
tree implemented by binary search.  Since $n>C/W$, each of these values
will occupy their own cache line, so the cache only has room for $C/W$
of these.

Furthermore, once binary search has reduced the search range down
to a size less than or equal to $W$, the subarray that remains to be
searched will occupy at most one or two cache lines. Thus, on average,
we expect binary search to incur roughly $\log n -\log(C/W) - \log W + 1 =
\log n - \log C + 1$ cache misses. 

The Intel 4790K has 8MB of L3 cache, which can store $C/W=125$K cache lines.



\subsubsection{Branch-Free Binary Search}

\subsection{Eytzinger}

\subsubsection{Na\"{\i}ve Code}

\subsubsection{Branch-Free Code}

\subsubsection{Branch-Free Code with Explicit Prefetching}

\subsection{BTree}





\section{Lessons Learned}

\subsection{Predictable memory accesses can be better than blocked
            memory accesses}

\subsection{For big data, branchy code can be the best prefetcher}

\subsection{A theoretical model}

This model is bullshit, in the sense that it doesn't explain why Eytzinger
is so much faster than b-tree layouts.  Ultimately, I think it's really just about constants in the code.


From our findings, we can design a theoretical model of computation of
a machine that has an (infinite but slow) external memory and a finite
(but fast) internal memory.  This model has the following parameters:

\begin{enumerate}

  \item Block size, $B$. The external memory is partitioned into equal
    blocks of size $B$. When a word is loaded from external memory,
    the entire block is loaded into the fast internal memory.

  \item Cache size, $C \gg B$.  This is the size of the internal
    memory. Before any computation can be done on data from external
    memory, it must be loaded into the fast internal memory.

  \item Latency, $L$. This is the minimum time that must elapse from the
    time the processor requests an external memory address until the block
    containing that address is loaded into internal memory.  The processor
    can make such requests in a non-blocking asynchronous manner, so that
    it can issue several requests in succession even before the first one
    completes.

    An algorithm that reads $t$ memory blocks one after the other (and
    not requesting the $(i+1)$th block before the $i$th block is read
    will have a running time of $tL$.

  \item Transfer rate, $T \ge 1/L$.  This is the maximum number of
    blocks that can be moved from external to internal memory per time
    unit.  Blocks are moved from external to internal memory in the order
    they are requested and as fast as the transfer rate and latency allow.

    An algorithm that immediately requests $t$ memory blocks and then
    completes once these $t$ blocks are available will have a running
    time of $\max\{L,t/T\}$.  During the first $L$ time units no data
    arrives. At the end of $L$ time units, the first $TL$ blocks arrive
    and then the remaining $t-TL$ blocks arrive as quickly as possible,
    in $(t-TL)/T=t/T -L$ time units.

\end{enumerate}

For example, with transfer rate $T=3$ and latency $L=1$, if the processor
requests three distinct blocks, then each block will arrive after a
delay of $L=1$.  If the processor requests 5 distinct blocks, then
the first three blocks will arrive after a delay of $L=1$. These first
three requests saturate the tranfer rate for the first time unit, so
the fourth and fifth block will only arrive after a delay of $L+1/3=4/3$
and $L+2/3=5/3$, respectively.

In this model, the running time of a single search using a b-tree that
stores $B$ keys per node is $L\lceil\log_{B+1}(n+1)\rceil$, since a
b-tree search consists of $\lceil\log_{B+1}(n+1)\rceil$ rounds. Each
round starts by loading a block and the next round does not begin until
that block has been read into internal memory and searched.

Note that, if $T$ is greater than or equal to $2$, then there is an
immediate way to speedup b-tree search. At the same time a block is
requested, one can request $k=\lfloor (T-1)L\rfloor$ randomly-chosen
children of that block.  The block itself, and these children, will
arrive in main memory at the same time.  

The cost of a single search in an Eytzinger array with explicit
prefetching is calculated as follows:  The first $\log B$ steps in the
search access elements in the first block, so these begin after a delay
of $L$. These steps issue $\log B$ prefetch operations


\end{document}


